{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e391792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys, torch\n",
    "# from pytorch_lightning import LightningModule\n",
    "# import src.utils as utils\n",
    "from pathlib import Path\n",
    "# /home/kan/ML_application/s4/outputs/2025-04-21/11-19-18/checkpoints/\n",
    "torch.set_printoptions(\n",
    "    threshold=float('inf'),      # すべての要素を表示\n",
    "    precision=10,                # 小数点以下10桁まで表示\n",
    "    linewidth=10**4              # 1行に表示する文字数（折り返し防止）\n",
    ")\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "fig_size_horizontal=15\n",
    "\n",
    "def is_array(obj):\n",
    "    return isinstance(obj, (list, np.ndarray))\n",
    "def DESN_observer(\n",
    "        u_ex, dim_rv, x_init=None, win=None, w=None, xinit_seed=0,winseed=0,wseed=0,\n",
    "        rho=0.5,w_norm=True, sign=1, densin=0.1,density=0.5, activation=\"tanh\", verbose=False):\n",
    "    dim_u_ex = u_ex.shape[0] # Obtain the dimension of input and output\n",
    "    Time_leng = u_ex.shape[1]\n",
    "\n",
    "    if is_array(x_init)==False:\n",
    "        np.random.seed(xinit_seed)\n",
    "        x_init=np.random.uniform(-1.0, 1.0, dim_rv)\n",
    "    if is_array(win)==False:\n",
    "        np.random.seed(winseed)\n",
    "        win = np.random.normal(\n",
    "            loc=0, scale=1.0/dim_rv**0.5, size=(dim_rv, dim_u_ex), ) * (np.random.rand(dim_rv, dim_u_ex)<densin)\n",
    "        # win = (2*np.random.rand(dim_rv,dim_u_ex)-1) * (np.random.rand(dim_rv, dim_u_ex)<densin)\n",
    "    if is_array(w)==False:\n",
    "        np.random.seed(wseed)\n",
    "        w = (2*np.random.rand(dim_rv,dim_rv)-1) * (np.random.rand(dim_rv,dim_rv)<density)\n",
    "    \n",
    "    eig = np.linalg.eigvals(w)\n",
    "    w = sign*w if dim_rv==1 or w_norm==False else rho*sign*w / np.max( np.abs(eig) )\n",
    "    # print(\"w=\",w, \" eig=\",eig)\n",
    "    # print(\"win\", win.shape, \" vars\", np.var(win.T[0]), np.var(win.T[1]))\n",
    "    \n",
    "    states_series = np.zeros((dim_rv, Time_leng))\n",
    "    states_series[:,0]=x_init\n",
    "    for t in range(1,Time_leng):\n",
    "        # print(\"time\", t)\n",
    "        # print(\"internal weight\", w.shape)\n",
    "        # # print(w)\n",
    "        # print(\"states_series[:,t-1:t]\", states_series[:,t-1:t].shape)\n",
    "        # print(states_series[:,t-1:t])\n",
    "        # print(\"win\", win.shape)\n",
    "        # # print(win)\n",
    "        # print(\"u_ex[:, t-1:t]\", u_ex[:, t-1:t].shape)\n",
    "        # print(u_ex[:, t-1:t])\n",
    "        if activation == \"lin\":\n",
    "            states_series[:,t:t+1] = w @ states_series[:,t-1:t] + win @ u_ex[:, t-1:t]\n",
    "        elif activation == \"tanh\":\n",
    "            states_series[:,t:t+1] = np.tanh(w @ states_series[:,t-1:t] + (win @ u_ex[:, t-1:t]) )\n",
    "        else:\n",
    "            print(\"Error!!! No such activation function.\")\n",
    "            sys.exit()    \n",
    "    return states_series\n",
    "\n",
    "def rank_svd(A, thresh='No', rcond=1e-15,):\n",
    "    dim_A = np.min(A.shape)\n",
    "\n",
    "    if thresh=='N':\n",
    "        finfo = np.finfo(A.dtype).eps #if finfo is None else finfo  # douBle=2^16\n",
    "        u_, sigma_, v_ = np.linalg.svd(A, full_matrices=False)\n",
    "        sigmax = sigma_.max()\n",
    "        # print(\"rank_svd\", A, sigma_, sigmax)\n",
    "        rcond_=(sigmax) * dim_A * finfo\n",
    "        index = np.where(sigma_ > rcond_)[0]\n",
    "        rank=index.shape[0]\n",
    "        rcond = rcond_/(sigmax)\n",
    "    else:\n",
    "        rank=np.linalg.matrix_rank(A)\n",
    "    return rank, rcond\n",
    "\n",
    "def MC_0ex_inv(\n",
    "        N, X, Z, maxtau=51, Two=10**4, T=10**6,\n",
    "        thresh='No', rcond=1e-15, debug=True):\n",
    "    taus = np.arange(maxtau)# for MF\n",
    "    MF = []\n",
    "    print(\"Z and X shapes\", Z.shape, X.shape)\n",
    "    X, Z = X[Two:], Z.reshape(-1, 1)\n",
    "    Z2 = (Z[Two:].T @ Z[Two:]).reshape(-1)[0]\n",
    "    print(\"Z and X shapes\", Z.shape, X.shape, \" Z2!\", Z2)\n",
    "    B = X.T @ X\n",
    "    B = B.astype(dtype=np.complex128)\n",
    "    rank, rcond = rank_svd(B, thresh=thresh, rcond=rcond)\n",
    "    B_1=np.linalg.pinv(B, rcond=rcond)\n",
    "    rank_B1, _ = rank_svd(B_1, thresh=thresh)\n",
    "    print(\"rankB1\", rank_B1)\n",
    "    for tau in taus[1:]:\n",
    "        # print(Z[Two-tau:T+Two-tau].shape, X.shape)\n",
    "        ZX = Z[Two-tau:T+Two-tau].T @ X\n",
    "        ipc = ZX @ B_1 @ ZX.T /Z2\n",
    "        ipc=ipc.reshape(-1)[0]\n",
    "        if debug ==True:\n",
    "            print(tau,ipc)#, (1-rho**2)*rho**(2*tau-2))\n",
    "        MF.append(ipc)\n",
    "    MF = np.array(MF, dtype=np.complex128)\n",
    "    MC = np.sum(MF)\n",
    "    if debug ==True:\n",
    "        print(\"MC_0ex_inv:\")\n",
    "        print(\"rank: XX:=%d\"%(rank))\n",
    "        if T < 1000:\n",
    "            rank_X_XX_X, rcond = rank_svd(X@B_1@X.T, thresh=thresh)\n",
    "            print(\"rank: X_XX_X:=%d\"%(rank_X_XX_X))\n",
    "    return MC, MF, rank\n",
    "\n",
    "def MC_3exNocor(\n",
    "    N, lams, Two=int(1e4), maxtau=51, thresh='No', rcond=1e-15, debug=False\n",
    "    ):\n",
    "    # K=Two# Condition: K <= Two\n",
    "    # delays = np.arange(0,K)# accuracy for MC\n",
    "    taus = np.arange(1,maxtau) # for MF\n",
    "    \n",
    "    Cs_rho=0\n",
    "    MF_rho=[]\n",
    "    B=np.array([[1/(1-lams[i]*lams[j]) for i in range(N)] for j in range(N)])\n",
    "    B=B.astype(dtype=np.complex128)\n",
    "    rank, rcond = rank_svd(B, thresh=thresh, rcond=rcond)\n",
    "    \n",
    "    B_1=np.linalg.pinv(B, rcond=rcond)\n",
    "    # print(B) # print(B_1) # print(B@B_1)\n",
    "    for tau in taus:\n",
    "        tauind=tau-1\n",
    "        h=(lams**tauind).reshape((N,1))\n",
    "        ipc=h.T @ B_1 @ h\n",
    "        ipc=ipc.reshape(-1)[0]\n",
    "        if debug:\n",
    "            print(tauind,ipc)#, (1-rho**2)*rho**(2*tauind-2))\n",
    "        Cs_rho+=ipc\n",
    "        MF_rho.append(ipc)\n",
    "    if debug:\n",
    "        print(Cs_rho)\n",
    "    \n",
    "    return Cs_rho, np.array(MF_rho), rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87cacad",
   "metadata": {},
   "source": [
    "# get paramas and MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71e0f0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kan/ML_application/s4/outputs/2025-05-13/18-29-34/checkpoints/epoch=00-metric=0.5153.ckpt\n",
      "['', 'home', 'kan', 'ML_application', 's4', 'outputs', '2025-05-13', '18-29-34', 'checkpoints', 'epoch=00-metric=0.5153.ckpt']\n",
      "\n",
      "/home/kan/ML_application/s4/outputs/2025-05-13/18-29-34/checkpoints/last.ckpt\n",
      "['', 'home', 'kan', 'ML_application', 's4', 'outputs', '2025-05-13', '18-29-34', 'checkpoints', 'last.ckpt']\n",
      "\n",
      "/home/kan/ML_application/s4/outputs/2025-05-13/18-29-34/checkpoints/epoch=01-metric=0.5332.ckpt\n",
      "['', 'home', 'kan', 'ML_application', 's4', 'outputs', '2025-05-13', '18-29-34', 'checkpoints', 'epoch=01-metric=0.5332.ckpt']\n",
      "\n",
      "/home/kan/ML_application/s4/outputs/2025-05-13/18-29-34/checkpoints/epoch=02-metric=0.6884.ckpt\n",
      "['', 'home', 'kan', 'ML_application', 's4', 'outputs', '2025-05-13', '18-29-34', 'checkpoints', 'epoch=02-metric=0.6884.ckpt']\n",
      "\n",
      "/home/kan/ML_application/s4/outputs/2025-05-13/18-29-34/checkpoints/epoch=03-metric=0.7367.ckpt\n",
      "['', 'home', 'kan', 'ML_application', 's4', 'outputs', '2025-05-13', '18-29-34', 'checkpoints', 'epoch=03-metric=0.7367.ckpt']\n",
      "\n",
      "/home/kan/ML_application/s4/outputs/2025-05-13/18-29-34/checkpoints/epoch=04-metric=0.7688.ckpt\n",
      "['', 'home', 'kan', 'ML_application', 's4', 'outputs', '2025-05-13', '18-29-34', 'checkpoints', 'epoch=04-metric=0.7688.ckpt']\n",
      "\n",
      "/home/kan/ML_application/s4/outputs/2025-05-13/18-29-34/checkpoints/epoch=05-metric=0.7900.ckpt\n",
      "['', 'home', 'kan', 'ML_application', 's4', 'outputs', '2025-05-13', '18-29-34', 'checkpoints', 'epoch=05-metric=0.7900.ckpt']\n",
      "\n",
      "/home/kan/ML_application/s4/outputs/2025-05-13/18-29-34/checkpoints/epoch=06-metric=0.8169.ckpt\n",
      "['', 'home', 'kan', 'ML_application', 's4', 'outputs', '2025-05-13', '18-29-34', 'checkpoints', 'epoch=06-metric=0.8169.ckpt']\n",
      "\n",
      "/home/kan/ML_application/s4/outputs/2025-05-13/18-29-34/checkpoints/epoch=07-metric=0.8458.ckpt\n",
      "['', 'home', 'kan', 'ML_application', 's4', 'outputs', '2025-05-13', '18-29-34', 'checkpoints', 'epoch=07-metric=0.8458.ckpt']\n",
      "\n",
      "/home/kan/ML_application/s4/outputs/2025-05-13/18-29-34/checkpoints/epoch=08-metric=0.8673.ckpt\n",
      "['', 'home', 'kan', 'ML_application', 's4', 'outputs', '2025-05-13', '18-29-34', 'checkpoints', 'epoch=08-metric=0.8673.ckpt']\n",
      "\n",
      "/home/kan/ML_application/s4/outputs/2025-05-13/18-29-34/checkpoints/epoch=09-metric=0.8841.ckpt\n",
      "['', 'home', 'kan', 'ML_application', 's4', 'outputs', '2025-05-13', '18-29-34', 'checkpoints', 'epoch=09-metric=0.8841.ckpt']\n",
      "\n",
      "/home/kan/ML_application/s4/outputs/2025-05-13/18-29-34/checkpoints/epoch=10-metric=0.8929.ckpt\n",
      "['', 'home', 'kan', 'ML_application', 's4', 'outputs', '2025-05-13', '18-29-34', 'checkpoints', 'epoch=10-metric=0.8929.ckpt']\n",
      "\n",
      "/home/kan/ML_application/s4/outputs/2025-05-13/18-29-34/checkpoints/epoch=11-metric=0.8967.ckpt\n",
      "['', 'home', 'kan', 'ML_application', 's4', 'outputs', '2025-05-13', '18-29-34', 'checkpoints', 'epoch=11-metric=0.8967.ckpt']\n",
      "\n",
      "/home/kan/ML_application/s4/outputs/2025-05-13/18-29-34/checkpoints/epoch=12-metric=0.9003.ckpt\n",
      "['', 'home', 'kan', 'ML_application', 's4', 'outputs', '2025-05-13', '18-29-34', 'checkpoints', 'epoch=12-metric=0.9003.ckpt']\n",
      "\n",
      "/home/kan/ML_application/s4/outputs/2025-05-13/18-29-34/checkpoints/epoch=13-metric=0.9182.ckpt\n",
      "['', 'home', 'kan', 'ML_application', 's4', 'outputs', '2025-05-13', '18-29-34', 'checkpoints', 'epoch=13-metric=0.9182.ckpt']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_params(filepath):\n",
    "    ckpt = torch.load(filepath, map_location='cpu')\n",
    "    # print(ckpt.keys())\n",
    "    state_dict = ckpt['state_dict']\n",
    "    # print(state_dict.keys())\n",
    "    \n",
    "    # for key in state_dict.keys():\n",
    "    #     print(key)\n",
    "    lams_arr = []\n",
    "    log_dt_arr = []\n",
    "    for rep_layer in range(n_layer):\n",
    "        # print(\"rep_layer\", rep_layer)\n",
    "        log_dt = state_dict[\"model.layers.%d.layer.kernel.kernel.log_dt\"%rep_layer]\n",
    "        log_w_real = state_dict[\"model.layers.%d.layer.kernel.kernel.log_w_real\"%rep_layer]\n",
    "        w_imag = state_dict[\"model.layers.%d.layer.kernel.kernel.w_imag\"%rep_layer]\n",
    "        dts = torch.exp(log_dt) # (H)\n",
    "        # print(\"dts\", dts.shape)\n",
    "        # print(dts)\n",
    "        mindt, maxdt, meandt=torch.min(dts), torch.max(dts), torch.mean(dts)\n",
    "        # print(\"dts\",mindt, maxdt, meandt)\n",
    "        \n",
    "        w_real = -torch.exp(log_w_real)\n",
    "        # print(\"w_real\", w_real.shape)\n",
    "        # print(\"w_imag\", w_imag.shape)\n",
    "        w = w_real + 1j * w_imag\n",
    "        # print(\"w\",w)\n",
    "        \n",
    "        dtA = w*dt\n",
    "        A = torch.exp(dtA)[0] # (H N)\n",
    "        # print(\"A\", A.shape, A)\n",
    "        rho = torch.max(torch.abs(A))\n",
    "        # print(\"rho\", rho)\n",
    "        \n",
    "        log_dt_arr.append(log_dt), lams_arr.append(A)\n",
    "        # w *= rho/np.max(np.abs(lams))\n",
    "        # dtA = w * dts.unsqueeze(-1)  # (H N)\n",
    "        # A = torch.exp(dtA) # (H N)\n",
    "    return log_dt_arr, lams_arr\n",
    "\n",
    "base_dir = \"/home/kan/ML_application/s4/outputs/\"\n",
    "# max_depth = 2\n",
    "# base_dir_ = Path(base_dir)\n",
    "\n",
    "dt = [0.01, 1][0]\n",
    "n_layer = 6\n",
    "data_dict={}\n",
    "\n",
    "# for file in base_dir_.glob(\"*/*/checkpoints/*\"):\n",
    "#     if file.is_file():\n",
    "#         parent_path = file.parents[1]\n",
    "#         print(\"parent_path\",parent_path)\n",
    "#         print(\"file\",file)\n",
    "\n",
    "files = [\"2025-05-13/18-29-34/\"]\n",
    "for file in files:\n",
    "    dirpath = base_dir+file+\"checkpoints/\"\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(dirpath):\n",
    "        for file in files:\n",
    "            if file.endswith('.ckpt'):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "    \n",
    "    for filepath in file_paths:\n",
    "        print(filepath)\n",
    "        filepath_split = filepath.split(\"/\")\n",
    "        print(filepath_split)\n",
    "        log_dt_arr, lams_arr = get_params(filepath)\n",
    "        print(\"\")\n",
    "        data_dict[filepath] = [log_dt_arr, lams_arr]\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6fa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputname = [\"gauss\", \"uniform\"][1]\n",
    "shuff = [True, False][1]\n",
    "ex_input_scale1 = [0, 0.05, 0.1, 0.2, 0.5, 1][2]\n",
    "ex_input_scale2 = [0, 0.05, 0.1, 0.2, 0.5, 1][0]\n",
    "densin_shared = [True, False][0]\n",
    "activation = ['lin', 'tanh'][0]\n",
    "for rep, (key, val) in enumerate(data_dict.items()):\n",
    "    nrows, ncols = 1, 3\n",
    "    axis_wide, axis_high = 8.0, 6.0\n",
    "    fig_ratio = 1\n",
    "    wspace, hspace = 0.4, 0.35\n",
    "    fig_wide = ncols*(axis_wide+wspace)\n",
    "    fig_high = nrows*(axis_high+hspace)\n",
    "    fig_wide_size = fig_size_horizontal*fig_ratio\n",
    "    fig_high_size = fig_size_horizontal*fig_ratio*fig_high/fig_wide\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, \n",
    "                        figsize = (fig_wide_size, fig_high_size)\n",
    "                        )#, subplot_kw=dict(projection=\"polar\"))\n",
    "    fig.subplots_adjust(wspace=wspace, hspace=hspace)\n",
    "    title=key.split(\"/\")[-1]\n",
    "    print(title)\n",
    "    fig.suptitle(title, x=0.5, y=0.98, fontsize=20)\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "\n",
    "    print(\"key\", key)\n",
    "    dts_arr=val[0]\n",
    "    lams_arr=val[1]\n",
    "    # print(key, lams_arr)\n",
    "    axid = ax[0]\n",
    "    # axid.set_title('%s'%key.split(\"/\")[0], x= -0.5, y=0.5, )\n",
    "    for rep_layer in range(n_layer):\n",
    "        dts = dts_arr[rep_layer]\n",
    "        dts = dts[np.argsort(np.abs(dts))]\n",
    "        axid.plot(\n",
    "            dts,\n",
    "            # label=\"num\", #+ noisename,\n",
    "            lw=5,\n",
    "            )\n",
    "    axid.grid()\n",
    "    \n",
    "    axid = ax[1]\n",
    "    axid.set_xlabel('indices')\n",
    "    # axid.set_ylim(-10**3, 10**3)\n",
    "    # axid.set_ylim(-0, 1.1)\n",
    "    # axid.set_yticks([0, 0.5, 1.0])\n",
    "    axid.set_ylabel('eigenvalue')\n",
    "    axid.grid()\n",
    "    for rep_layer in range(n_layer):\n",
    "        lams = np.abs(lams_arr[rep_layer])\n",
    "        lams = lams[np.argsort(np.abs(lams))]\n",
    "        \n",
    "        # print(lams)\n",
    "        axid.plot(\n",
    "            lams,\n",
    "            # label=\"num\", #+ noisename,\n",
    "            lw=5\n",
    "            )\n",
    "    \n",
    "    axid = ax[2]\n",
    "    axid.set_xlabel(r'$\\tau$')\n",
    "    # axid.set_ylim(-10**3, 10**3)\n",
    "    # axid.set_ylim(-0, 1.1)\n",
    "    # axid.set_yticks([0, 0.5, 1.0])\n",
    "    axid.set_ylabel('MF')\n",
    "    axid.grid()\n",
    "    for rep_layer in range(n_layer):\n",
    "        lams = lams_arr[rep_layer]\n",
    "        dim_rv = lams.shape[0]\n",
    "        print(dim_rv, lams.shape)\n",
    "        # mc, mf, rank =MC_3exNocor(dim_rv, lams.numpy(), Two=int(1e4), maxtau=51, debug=False)\n",
    "        \n",
    "        #################################\n",
    "        Two = 5*10**2\n",
    "        T = 10**4\n",
    "        np.random.seed(2)\n",
    "        vsigma=1\n",
    "        u = 2*np.random.rand(Two+T)-1\n",
    "        u *= ex_input_scale1/np.std(u)\n",
    "        u_ex = np.array([u])\n",
    "        # np.random.seed(seedin+1)\n",
    "        # win = (2*np.random.rand(dim_rv,2)-1) * (np.random.rand(dim_rv,2)<densin)\n",
    "        # win = np.random.normal(dim_rv,2) * (np.random.rand(dim_rv, 2)<densin)\n",
    "        # np.random.seed(w_index+3)\n",
    "        if densin_shared:\n",
    "            w1 = (2*np.random.rand(dim_rv)-1) * (np.random.rand(dim_rv)<1.0)\n",
    "            win = np.array([w1]).T\n",
    "        # else:\n",
    "        #     win = np.random.normal(\n",
    "        #         loc=0, scale=1.0/dim_rv**0.5, size=(dim_rv, 2), ) * (np.random.rand(dim_rv, 2)<1.0)\n",
    "            # print(\"win\", win.shape, \" vars\", np.var(win.T[0]), np.var(win.T[1]))\n",
    "        \n",
    "        maxtau=200\n",
    "        states_series=DESN_observer(\n",
    "            u_ex, dim_rv, \n",
    "            x_init=np.zeros(dim_rv), w = np.diag(lams), win = win,\n",
    "            rho=1.0, w_norm=True,# False if lam_name==\"gauss\" else True,\n",
    "            densin = 1.0, density = None,\n",
    "            activation = activation,)\n",
    "\n",
    "        rcond=1e-15\n",
    "        # print(np.var(u), np.var(v))\n",
    "        MC_num, MFs_num, ranks_num = MC_0ex_inv(\n",
    "            dim_rv, states_series.T, u, maxtau=maxtau, Two=Two, T=T, thresh='N', rcond=rcond,\n",
    "            debug=False,\n",
    "            )\n",
    "        axid.plot(\n",
    "            MFs_num,\n",
    "            # label=\"num\", #+ noisename,\n",
    "            lw=5\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e5952",
   "metadata": {},
   "source": [
    "# Experiments that do not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6c3ade9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<wandb.apis.public.api.Api object at 0x7fa2f0e0b3d0>\n",
      "x2hzde8t\n",
      "<Runs jingchuan0guan-the-university-of-tokyo-hospital/hippo>\n",
      "0 bxdoprfi\n",
      "<Run jingchuan0guan-the-university-of-tokyo-hospital/hippo/bxdoprfi (finished)>\n",
      "<Artifact QXJ0aWZhY3Q6MTU4MjYzMzQyOQ==>\n",
      "1 wxox9i81\n",
      "<Run jingchuan0guan-the-university-of-tokyo-hospital/hippo/wxox9i81 (finished)>\n",
      "<Artifact QXJ0aWZhY3Q6MTY0NTIxMTYzMQ==>\n",
      "2 u4c4hkof\n",
      "<Run jingchuan0guan-the-university-of-tokyo-hospital/hippo/u4c4hkof (finished)>\n",
      "<Artifact QXJ0aWZhY3Q6MTY0OTMyOTU1MQ==>\n"
     ]
    }
   ],
   "source": [
    "import torch, wandb\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf, precision=10, linewidth=10**4)\n",
    "torch.set_printoptions(threshold=np.inf, precision=10, linewidth=10**4)\n",
    "api = wandb.Api()\n",
    "print(api)\n",
    "\n",
    "project_name = 'hippo'\n",
    "run_id = wandb.run.id  # 現在のランIDを取得\n",
    "print(run_id)\n",
    "\n",
    "runs = api.runs(project_name)\n",
    "print(runs)\n",
    "\n",
    "for idx, run in enumerate(runs):\n",
    "    print(idx, run.id)\n",
    "    print(run)\n",
    "    metrics = run.history()\n",
    "    # for rep, met in enumerate(metrics):\n",
    "    #     print(met)\n",
    "    \n",
    "    artifacts = run.logged_artifacts()  # ランに関連する全てのアーティファクト\n",
    "    for artifact in artifacts:\n",
    "        print(artifact)\n",
    "        if 'model' in artifact.name.lower():  # モデルアーティファクトを特定する\n",
    "            print(f\"Artifact Name: {artifact.name}\")\n",
    "            print(f\"Artifact Type: {artifact.type}\")\n",
    "            print(f\"Artifact Metadata: {artifact.metadata}\")  # メタデータにモデルに関する情報が含まれていることがあります\n",
    "    if idx==2:\n",
    "        break\n",
    "\n",
    "# run = api.run(\"ユーザー名/プロジェクト名/ランID\")\n",
    "# artifact = run.use_artifact(\"モデル名:latest\") #（例：last.ckpt）\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "# model = MyModel.load_from_checkpoint(artifact_dir + \"/last.ckpt\") # モデル読み込み（Lightningなど使ってるなら）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf86f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hydra.main(config_path=\"./configs/model/\", config_name=\"s4d\", version_base=None)\n",
    "# def get_model(model_config: DictConfig):\n",
    "#     print(model_config)  # DictConfigとしてアクセス可能\n",
    "#     for key in model_config.keys():\n",
    "#         print(key, model_config[key])\n",
    "#     model = utils.instantiate(registry.model, model_config)#.load_from_checkpoint(\"path/to/model.ckpt\")\n",
    "#     print(model)\n",
    "    \n",
    "#     state_dict = model.state_dict()\n",
    "#     for k, v in state_dict.items():\n",
    "#         print(k, v.shape)\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('/home/kan/ML_application/s4/src')\n",
    "# layer_key = [\"s4d\", ][0]\n",
    "# print(registry.model[\"model\"])\n",
    "# print(registry.layer[layer_key])\n",
    "# print(registry.model[\"model\"](layer=registry.layer[layer_key]))\n",
    "# ckpt ファイルをロード\n",
    "# model = utils.instantiate(registry.model, layer=\"s4d\", )\n",
    "# registry.model[\"model\"](layer=layer).load_from_checkpoint(\"path/to/model.ckpt\")\n",
    "\n",
    "# for root, dirs, files in os.walk(base_dir):\n",
    "#     # 今の階層の深さを数える\n",
    "#     depth = root[len(base_dir):].count(os.sep)\n",
    "#     print(depth)\n",
    "#     if depth > max_depth:\n",
    "#         # さらに深い階層の探索をやめる\n",
    "#         dirs[:] = []\n",
    "#         continue\n",
    "\n",
    "#     for file in files:\n",
    "#         print(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf8bd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kan/ML_application/s4/outputs/2025-04-20/22-59-56\n",
      "/home/kan/ML_application/s4/outputs/2025-04-20/22-59-56/checkpoints/last.ckpt\n",
      "outputs/2025-04-20/22-59-56/.hydra\n",
      "cfg {'train': {'seed': 0, 'interval': 'epoch', 'monitor': 'val/accuracy', 'mode': 'max', 'ema': 0.0, 'test': False, 'debug': False, 'ignore_warnings': False, 'state': {'mode': None, 'chunk_len': None, 'overlap_len': None, 'n_context': 0, 'n_context_eval': '${.n_context}'}, 'sweep': None, 'group': None, 'benchmark_step': False, 'benchmark_step_k': 1, 'benchmark_step_T': 1, 'checkpoint_path': None, 'visualizer': 'filters', 'disable_dataset': False}, 'wandb': {'project': 'hippo', 'group': '', 'job_type': 'training', 'mode': 'online', 'save_dir': None, 'id': None}, 'trainer': {'accelerator': 'cuda', 'devices': [0, 1, 2, 3, 4, 5, 6, 7], 'accumulate_grad_batches': 1, 'max_epochs': 100, 'gradient_clip_val': 0.0, 'log_every_n_steps': 10, 'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'enable_progress_bar': True}, 'loader': {'batch_size': 16, 'num_workers': 4, 'pin_memory': True, 'drop_last': True, 'train_resolution': 1, 'eval_resolutions': [1]}, 'dataset': {'_name_': 'pathfinder', 'resolution': 128, 'sequential': True, 'tokenize': False, 'pool': 1, 'val_split': 0.1, 'test_split': 0.1, 'seed': 42, '__l_max': '${eval:${.resolution}**2 // ${.pool}**2}'}, 'task': {'_name_': 'base', 'loss': 'cross_entropy', 'metrics': ['accuracy'], 'torchmetrics': None}, 'optimizer': {'_name_': 'adamw', 'lr': 0.0005, 'weight_decay': 0.0}, 'scheduler': {'_name_': 'plateau', 'mode': '${train.mode}', 'factor': 0.2, 'patience': 40, 'min_lr': 0.0}, 'encoder': 'linear', 'decoder': {'_name_': 'sequence', 'mode': 'pool'}, 'model': {'layer': {'_name_': 's4d', 'd_state': 64, 'channels': 1, 'bidirectional': False, 'activation': 'gelu', 'postact': None, 'dropout': '${..dropout}', 'imag_scaling': 'linear', 'dt_min': 0.001, 'dt_max': 0.1, 'lr': 0.001, 'n_ssm': 1}, '_name_': 'model', 'prenorm': True, 'transposed': True, 'n_layers': 6, 'd_model': 256, 'residual': 'R', 'pool': {'_name_': 'sample', 'stride': 1, 'expand': 1}, 'norm': 'batch', 'dropout': 0.0}, 'callbacks': {'learning_rate_monitor': {'logging_interval': '${train.interval}'}, 'timer': {'step': True, 'inter_step': False, 'epoch': True, 'val': True}, 'params': {'total': True, 'trainable': True, 'fixed': True}, 'model_checkpoint': {'monitor': '${train.monitor}', 'mode': '${train.mode}', 'save_top_k': 1, 'save_last': True, 'dirpath': 'checkpoints/', 'filename': '${train.monitor}', 'auto_insert_metric_name': False, 'verbose': True}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_649803/159137829.py:14: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=cfg_path):\n"
     ]
    },
    {
     "ename": "InstantiationException",
     "evalue": "Cannot instantiate config of type type.\nTop level config must be an OmegaConf DictConfig/ListConfig object,\na plain dict/list, or a Structured Config class or instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInstantiationException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m cfg_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(relative_path)\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/.hydra\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg_path)\n\u001b[0;32m---> 59\u001b[0m \u001b[43mcheck_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[75], line 23\u001b[0m, in \u001b[0;36mcheck_model\u001b[0;34m(model_name, cfg_name, cfg_path, model_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m ModelClass \u001b[38;5;241m=\u001b[39m hydra\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_class(registry\u001b[38;5;241m.\u001b[39mmodel[model_name])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# model = ModelClass(cfg)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# model = instantiate(cfg.model) # registry.model,\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43minstantiate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModelClass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# モデル構造のインスタンス化（この時点でランダム初期化）\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(model))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.20/lib/python3.8/site-packages/hydra/_internal/instantiate/_instantiate2.py:253\u001b[0m, in \u001b[0;36minstantiate\u001b[0;34m(config, *args, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m instantiate_node(\n\u001b[1;32m    250\u001b[0m         config, \u001b[38;5;241m*\u001b[39margs, recursive\u001b[38;5;241m=\u001b[39m_recursive_, convert\u001b[38;5;241m=\u001b[39m_convert_, partial\u001b[38;5;241m=\u001b[39m_partial_\n\u001b[1;32m    251\u001b[0m     )\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InstantiationException(\n\u001b[1;32m    254\u001b[0m         dedent(\n\u001b[1;32m    255\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124m            Cannot instantiate config of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(config)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124m            Top level config must be an OmegaConf DictConfig/ListConfig object,\u001b[39m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124m            a plain dict/list, or a Structured Config class or instance.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    259\u001b[0m         )\n\u001b[1;32m    260\u001b[0m     )\n",
      "\u001b[0;31mInstantiationException\u001b[0m: Cannot instantiate config of type type.\nTop level config must be an OmegaConf DictConfig/ListConfig object,\na plain dict/list, or a Structured Config class or instance."
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import LightningModule\n",
    "# import src.utils as utils\n",
    "from pathlib import Path\n",
    "\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "# from hydra.utils import instantiate\n",
    "\n",
    "from src.utils import registry, instantiate\n",
    "\n",
    "def check_model(model_name=\"model\", cfg_name=\"config\", cfg_path=\"./configs/model/\",  model_path=\"\", ):\n",
    "    with initialize(config_path=cfg_path):\n",
    "        cfg = compose(config_name=cfg_name)\n",
    "    print(\"cfg\", cfg)\n",
    "    OmegaConf.set_struct(cfg, False)\n",
    "    # cfg.model._target_ = \"torch.nn.Identity\" \n",
    "    ModelClass = hydra.utils.get_class(registry.model[model_name])\n",
    "    # model = ModelClass(cfg)\n",
    "    \n",
    "    # model = instantiate(cfg.model) # registry.model,\n",
    "    model = instantiate(registry.model, cfg) # モデル構造のインスタンス化（この時点でランダム初期化）\n",
    "    print(model)\n",
    "    print(\"type\", type(model))\n",
    "    ckpt = torch.load(model_path, map_location=\"cpu\")# チェックポイントからstate_dictを読み込み\n",
    "    # state_dict = {k.replace(\"model.\", \"\"): v for k, v in ckpt[\"state_dict\"].items()}\n",
    "    # model.load_state_dict(state_dict)\n",
    "    print(\"Checkpoint keys:\", ckpt.keys())  # 追加\n",
    "    \n",
    "    state_dict = ckpt.get(\"state_dict\", ckpt)\n",
    "    model.load_state_dict(state_dict)\n",
    "    # if \"state_dict\" in ckpt:\n",
    "    #     state_dict = ckpt[\"state_dict\"]\n",
    "    # else:\n",
    "    #     state_dict = ckpt\n",
    "    # model.load_state_dict(state_dict)\n",
    "    \n",
    "    # for key in cfg.keys():\n",
    "    #     print(key, cfg[key])\n",
    "    # model = utils.instantiate(registry.model, cfg).load_from_checkpoint(model_path)\n",
    "\n",
    "    print(model)\n",
    "    \n",
    "# /home/kan/ML_application/s4/outputs/2025-04-21/11-19-18/checkpoints/\n",
    "\n",
    "base_dir = \"/home/kan/ML_application/s4/outputs/\"\n",
    "max_depth = 2\n",
    "\n",
    "base_dir_ = Path(base_dir)\n",
    "for file in base_dir_.glob(\"*/*/checkpoints/*\"):\n",
    "    if file.is_file():\n",
    "        parent_path = file.parents[1]\n",
    "        print(parent_path)\n",
    "        print(file)\n",
    "        relative_path = parent_path.parts[parent_path.parts.index(\"outputs\"):]  # s4 以降の部分を抽出\n",
    "        cfg_path = \"/\".join(relative_path)+ \"/.hydra\"\n",
    "        print(cfg_path)\n",
    "        check_model(model_name=\"model\", cfg_path=cfg_path, model_path=file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de08d7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
